{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lec04-3-regression-house-price.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TSush6PC045-"},"source":["## Predicting house prices: a regression example"]},{"cell_type":"markdown","metadata":{"id":"94JwhusV1HhY"},"source":["* Predicting a continuous value instead of a discrete label"]},{"cell_type":"markdown","metadata":{"id":"lrucavSI1Vgd"},"source":["> ### The Boston Housing Price dataset"]},{"cell_type":"markdown","metadata":{"id":"74bUmfZC1Yl1"},"source":["* We want to predict the median price of homes in a given Boston suburb in the mid-1970s, given the crime rate, the local property tax rate, and so on.\n","* It has relatively few data points: only 506 (404 training samples and 102 test samples).\n","* Each feature in the input has a different scale.\n","  * For instance, some values are proportions, which take values between 0 and 1; others take values between 1 and 12, and so on."]},{"cell_type":"code","metadata":{"id":"XCSQu0Ww2KF7"},"source":["from tensorflow.keras.datasets import boston_housing\n","\n","(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMKuzm4n2YGF"},"source":["train_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pYYBon2atAU1"},"source":["train_data[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7r_aGQz-2aTm"},"source":["test_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cCrJbCi2bPW"},"source":["train_targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L21r5du52eyL"},"source":["> ### Preparing the data"]},{"cell_type":"markdown","metadata":{"id":"2Oi3GNHy2lDL"},"source":["* It would be problematic to feed into a neural network values that all take wildly different ranges.\n","* Let's do feature-wise normalization\n","  * For each feature, we subtract the mean of the feature and divide by the standard deviation.\n","  * Then, the feature is centered around 0 and has a unit standard deviation."]},{"cell_type":"code","metadata":{"id":"3euPc75V3HD0"},"source":["mean = train_data.mean(axis=0)\n","std = train_data.std(axis=0)\n","\n","train_data -= mean\n","train_data /= std\n","\n","test_data -= mean\n","test_data /= std\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pMBeQFh3Rc2"},"source":["* Note that the quantities used for normalizing the test data are computed using the training data.\n","* NEVER use any quantity computed on the test data, even for something as simple as data normalization."]},{"cell_type":"markdown","metadata":{"id":"4O9BOxZN3dG6"},"source":["> ### Building the network"]},{"cell_type":"code","metadata":{"id":"o8bxOuzw3gAx"},"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","\n","def build_model():\n","  model = models.Sequential()\n","  model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n","  model.add(layers.Dense(64, activation='relu'))\n","  model.add(layers.Dense(1))\n","  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LWXdEx1632fA"},"source":["* This network ends with a single unit and no activation (it is called a linear layer).\n","* `mse` loss \n","  * mean squared error, the square of the difference between the predictions and the targets\n","* `mae` for monitoring\n","  * mean absolute error, the absolute error of the difference between the predictions and the targets"]},{"cell_type":"markdown","metadata":{"id":"5QyXao524Uuj"},"source":["> ### Validation with k-fold cross validation technique"]},{"cell_type":"markdown","metadata":{"id":"gR0aZo3C4a-7"},"source":["* Since we have few data points, the validation set would be very small if we randomly split the data into a training set and a validation set.\n","  * It means that the validation scores might change a lot depending on which data points we chose for the validation.\n","  * We can say that the validation scores might have a high *variance* with regard to the validation split.\n","  \n","* The best practice in such situations is to use *k-fold cross-validation*.\n","  * It consists of splitting the available data into *k* partitions, instantiating *k* identical models, and training each one of *k-1* partitions while evaluating on the remaining partition.\n","  * The validation score for the model used is then the average of the *k* validation scores obtained.\n","  \n","  <img src=\"https://drive.google.com/uc?id=13ND0yDLHrn1GmDKJ1TjVg21jbeji8DAt\" width=\"800\">\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lCAw8XBA6Qgh"},"source":["import numpy as np\n","\n","k = 4\n","num_val_samples = len(train_data) // k\n","num_epochs = 300\n","all_mae_histories = []\n","\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data = train_data[i*num_val_samples: (i+1)*num_val_samples]\n","  val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n","  \n","  partial_train_data = np.concatenate([train_data[:i*num_val_samples],\n","                                       train_data[(i+1)*num_val_samples:]],\n","                                      axis=0)\n","  partial_train_targets = np.concatenate([train_targets[:i*num_val_samples],\n","                                          train_targets[(i+1)*num_val_samples:]],\n","                                         axis=0)\n","  \n","  model = build_model()\n","  history = model.fit(partial_train_data, \n","                      partial_train_targets,\n","                      validation_data=(val_data, val_targets),\n","                      epochs=num_epochs,\n","                      batch_size=16, \n","                      verbose=0)\n","  mae_history = history.history['val_mae']\n","  all_mae_histories.append(mae_history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67GBGLKc2Vhk"},"source":["average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28qg2WkS2Som"},"source":["* Plotting validation scores"]},{"cell_type":"code","metadata":{"id":"y6qMITbx9j7P"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1, len(average_mae_history)+1), average_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuQCZ9p19tzm"},"source":["def smooth_curve(points, factor=0.9):\n","  smoothed_points = []\n","  for point in points:\n","    if smoothed_points:\n","      previous = smoothed_points[-1]\n","      smoothed_points.append(previous*factor + point*(1-factor))\n","    else:\n","      smoothed_points.append(point)\n","  return smoothed_points\n","\n","smooth_mae_history = smooth_curve(average_mae_history[10:])\n","\n","plt.plot(range(1, len(smooth_mae_history)+1), smooth_mae_history)\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation MAE')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgB939Z_BrmM"},"source":["> ### Exercise"]},{"cell_type":"markdown","metadata":{"id":"YtVvyf4PCQbs"},"source":["* We found that validation MAE stops improving at a some point.\n","* Write a code to train a final production model on all of the training data and then look at its performance on the test data."]},{"cell_type":"code","metadata":{"id":"Sqs9yBLhw_oX"},"source":[""],"execution_count":null,"outputs":[]}]}